# FHIR Data Processing Pipeline

This project provides a pipeline to process and transform patient data in the FHIR (Fast Healthcare Interoperability Resources) format. The transformed data is saved in a more structured and accessible format (e.g., Parquet) for use by analytics teams for further processing and visualisation.

---

## ğŸ“ Project Structure

```
fhir-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Raw data received in FHIR format
â”‚   â””â”€â”€ processed/          # Processed data in tabular format
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py           # Configuration variables
â”‚   â”œâ”€â”€ ingestion.py        # Module to load and parse FHIR data
â”‚   â”œâ”€â”€ transformation.py   # Module to clean and transform data
â”‚   â”œâ”€â”€ storage.py          # Module to save data
â”‚   â””â”€â”€ main.py             # Main execution file for the pipeline
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_transformation.py  # Unit tests for transformation logic
â”‚   â””â”€â”€ test_ingestion.py       # Unit tests for ingestion logic
â”œâ”€â”€ Dockerfile              # Docker configuration
â”œâ”€â”€ Makefile                # Simplifies common commands
â”œâ”€â”€ requirements.txt        # Python dependencies
â””â”€â”€ README.md               # Project documentation
```
---

## âš™ï¸ Setup

Before running the pipeline, ensure you have the necessary environment and dependencies configured.

---

### ğŸ³ Option 1: Set Up Docker Environment

This project includes Docker support to simplify setup and execution.

#### Step 1: Build the Docker Image
```bash
`make docker-build`
This creates a Docker image based on the Dockerfile.

#### Step 2: Run the Pipeline in Docker

`make docker-run`
This executes the pipeline inside a container. The data/ directory is mounted, so processed data is stored locally in data/processed/.

#### Step 3: Run Unit Tests in Docker

`make docker-test`
Runs tests using pytest in a Docker container.

ğŸ’» Option 2: Set Up Local Environment (Virtualenv)
If you prefer not to use Docker:

Step 1: Install Runtime Dependencies

`make install`
Creates a venv virtual environment and installs dependencies.

Step 2: Install Development Dependencies

`make dev-install`
Installs additional dev/test tools (like pytest).

Step 3: Run Unit Tests Locally

`make test`
Executes local tests via pytest.

Step 4: Run the Pipeline Locally

`make run`
Runs the full pipeline via python src/main.py.

ğŸ›¢ PostgreSQL Database Setup

The pipeline uses PostgreSQL to persist patient data. If you're using Docker, the database setup is automated via docker-compose.

#### Step 1: Start the PostgreSQL Service
`docker-compose up`
This will start the database container. The DB is accessible at localhost:5432.

####Â Step 2: Connect to the Database
Use psql to connect:

`psql -h localhost -U fhiruser -d patients`

Credentials:
User: fhiruser
Password: password
Database: patients

#### Step 3: View Table Contents
Once inside the psql CLI:

`SELECT * FROM patients;`

Exit with:

`\q`

ğŸ”„ How the Pipeline Works

    Ingestion
    ingestion.py loads raw FHIR JSON from data/raw/.
    Transformation
    transformation.py:
    Renames columns
    Converts data types
    Cleans & fills missing values
    Validation
    Ensures required fields are present and valid.
    Storage
    Saves cleaned data to:
    data/processed/ as Parquet
    PostgreSQL database (patients table)

ğŸ§ª Running Tests
Locally:
`make test`
In Docker:
`make docker-test`

ğŸ³ Docker Usage Summary
Build image: `make docker-build`
Run pipeline: `make docker-run`
Run tests: `make docker-test`

ğŸ“‹ Requirements
Python 3.10+
Docker (optional but recommended)
Make (for convenience)

ğŸ§° Docker Installation

Install Docker from the official site:
ğŸ‘‰ https://www.docker.com/products/docker-desktop

